[{"content":"Feature Engineering 1. Why do we need to apply normalization to numerical features? There are two common ways of normalization:\na. Min-Max Scaling\n$$X_{norm} = \\frac{X-X_{min}}{X_{max}-X_{min}}$$\nThis method can scale the data into a range of [0,1).\nb. Z-Score Normalization\n$$z = \\frac{x-\\mu}{\\rho}, \\quad \\rho=\\sqrt{\\sum\\frac{(x_i-\\mu)^2}{N}}$$\nThis method will scale the data and make the mean value and standard deviation of the new data become 0 and 1 respectively.\nWhen the scales of features are different, the gradients of weights of features can be very different, leading to a different \u0026rsquo;learning pace\u0026rsquo; of each weight, shown as a zig-zag on the gradient plot. Scaling features can make the \u0026rsquo;learning pace\u0026rsquo; of each weight become closer and let the model converge faster.\nScaling the features can be useful when the model uses gradient descent to update the parameters. Otherwise, it may not make a difference, e.g. when the model is a decision tree.\n2. How do we handle categorical features? We can use Ordinal Encoding, One-hot Encoding, and Binary Encoding based on the categorical feature itself. In deep learning, we can learn an embedding representation of a categorical feature, which can be seen as an advanced way of One-hot Encoding and Binary Encoding.\nWe can further encode categorical features with other features, e.g. statistical values of numerical features of each category, and target encoding.\n3. What is Feature Intersection? How do we intersect high-dimensional features? We can intersect different features and generate new features to improve the fitting ability of our model.\nFor high-dimensional features, we can first represent them in lower dimensions and then intersect them. A typical case is using eigenvectors to represent users and items in a recommender system and then intersecting the eigenvectors to represent the relationship between users and items.\n","permalink":"https://wp-zhang.github.io/posts/crack-machine-learning-interviews/","summary":"Feature Engineering 1. Why do we need to apply normalization to numerical features? There are two common ways of normalization:\na. Min-Max Scaling\n$$X_{norm} = \\frac{X-X_{min}}{X_{max}-X_{min}}$$\nThis method can scale the data into a range of [0,1).\nb. Z-Score Normalization\n$$z = \\frac{x-\\mu}{\\rho}, \\quad \\rho=\\sqrt{\\sum\\frac{(x_i-\\mu)^2}{N}}$$\nThis method will scale the data and make the mean value and standard deviation of the new data become 0 and 1 respectively.\nWhen the scales of features are different, the gradients of weights of features can be very different, leading to a different \u0026rsquo;learning pace\u0026rsquo; of each weight, shown as a zig-zag on the gradient plot.","title":"Crack Machine Learning Interviews"},{"content":"\nHi there~\nI\u0026rsquo;m Weipeng Zhang and you can call me Y.Paang (much easier to pronounce).\nI\u0026rsquo;m currently an MSDS student at Northeastern University, US. I have experience in machine learning, data modeling, and data engineering.\n","permalink":"https://wp-zhang.github.io/about/","summary":"Hi there~\nI\u0026rsquo;m Weipeng Zhang and you can call me Y.Paang (much easier to pronounce).\nI\u0026rsquo;m currently an MSDS student at Northeastern University, US. I have experience in machine learning, data modeling, and data engineering.","title":"About"}]