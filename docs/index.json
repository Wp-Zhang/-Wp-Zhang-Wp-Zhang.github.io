[{"content":"There are two very important types of relationships between products: substitute and complementary. Substitute products are those that are interchangeable while complementary products are those that might be purchased in addition. For example, when a user is looking at a T-shirt, substitute products are other different types of T-shirts and complementary products can be shorts, hoodies or jackets. Swing is designed for substitute relationships and Surprise is designed for complementary relationships.\nSwing The basic idea of Swing is that if user $A$ and user $B$ who both clicked item $i$ also clicked item $j$, then item $i$ and item $j$ are considered similar. The less similar $A$ and $B$ are, i.e. the smaller the number of items bought both by $A$ and $B$ is, the more similar $i$ and $j$ are.\n$$ s(i,j) = \\sum_{u\\in U_i \\cap U_j}\\sum_{v\\in U_i \\cap U_j}\\frac{1}{\\sqrt{|I_u|}} * \\frac{1}{\\sqrt{|I_v|}} * \\frac{1}{\\alpha + |I_u \\cap I_v|} $$\n$\\frac{1}{\\sqrt{|I_u|}}$ and $\\frac{1}{\\sqrt{|I_v|}}$ are weights to reduce the impact of active users and $\\alpha$ is the smoothing factor.\nSurprise Category-level The probability of $c_j$ is a related category for $c_i$ is defined as:\n$$ \\theta_{i,j} = p(c_{i,j}|c_j) = \\frac{N(c_{i,j})}{N(c_j)} $$\nwhere $N_{c_j}$ is the total purchases in category $c_j$, and $N_{c_{i,j}}$ is the number of times $c_j$ is purchased after $c_i$.\nProduct-level For each top related category, we can find top related products by:\n$$ s_1(i,j) = \\frac{\\sum_{u\\in U_i \\cap U_j} 1/(1+t_{uj} - t_{ui})}{|U_i|\\times |U_j|} $$\nwhere $t_{ui}$ and $t_{uj}$ denote the time that user $u$ bought item $i$ and item $j$. As the order in the product pairs is important, e.g. we can recommend phone cases to a user who just bought a new phone but we shouldn\u0026rsquo;t recommend phones to the user who just bought a new phone case, thus $t_{uj} \\ge t_{ui}$.\nCluster-level Since the item-level user co-purchasing data is extremely sparse, we can calculate the relevance score for items at the cluster level to alleviate this problem.\nLet $L(i)$ denote the cluster that item $i$ belongs to, which is calculated by the relevance score of items from Swing. Then the cluster-level relevance score can be calculated by:\n$$ s_2(i,j) = s_1(L(i), L(j)) $$\nCompute Surprise score The final Suprise score of a product pair is:\n$$ s_{i,j} = \\omega * s_1(i,j) + (1 - \\omega) * s_2(L(i),L(j)) $$\nwhere $\\omega$ is the mannually set combination weight.\nReferences [1] Xiaoyong Yang et al. \u0026ldquo;Large Scale Product Graph Construction for Recommendation in E-commerce.\u0026rdquo; ArXiv, abs/2010.05525.\n[2] Ruyi Luo et al. FunRec - Swing\n","permalink":"https://wp-zhang.github.io/posts/swing/","summary":"There are two very important types of relationships between products: substitute and complementary. Substitute products are those that are interchangeable while complementary products are those that might be purchased in addition. For example, when a user is looking at a T-shirt, substitute products are other different types of T-shirts and complementary products can be shorts, hoodies or jackets. Swing is designed for substitute relationships and Surprise is designed for complementary relationships.","title":"Retrieval 02: Swing and Surprise"},{"content":"UserCF UserCF uses ratings of the target item from top N similar users to predict the rating from the current user. There are mainly two steps:\nSteps 1. Calculate similarities between users. There are three common ways of calculating user similarities:\na. Jaccard Similarity: $$J_{u,v} = \\frac{|N(u) \\cap N(v)|}{|N(u) \\cup N(v)|}$$ $N(u)$ denotes the interacted item set of user $u$.\nb. Cosine Similarity: $$cos(u,v) = \\frac{u\\cdot v}{|u|\\cdot|v|}$$ $u$ and $v$ denote the rating vectors of two users respectively.\nc. Pearson Correlation Coefficient: $$Pearson_{u,v} = \\frac{\\sum_i(r_{ui} - \\overline r_u)(r_{vi} - \\overline r_v)}{\\sqrt{\\sum_i (r_{ui} - \\overline r_u)^2} \\sqrt{\\sum_i (r_{vi} - \\overline r_v)^2}}$$ $r_{ui}$ denotes rating of item $i$ from user $u$ and $\\overline r_u$ denotes the average rating of user $u$. Compared with Cosine Similarity, Pearson Correlation Coefficient subtracts users\u0026rsquo; ratings by their average rating to reduce the effect of users\u0026rsquo; rating biases.\n2. Use ratings from the top N similar users to predict the rating from the current user. The equation of calculating predicted rating of item $p$ by user $u$ is:\n$$R_{u,p} = \\frac{\\sum_{s \\in S}sim_{s,u} \\cdot R_{s,p}}{\\sum_{s \\in S}sim_{s,u}}$$\nSimilar to Pearson Correlation Coefficient, we can further reduce the effect of users\u0026rsquo; rating biases by substracting each user\u0026rsquo;s ratings by his/her average rating:\n$$R_{u,p} = \\overline R_u + \\frac{\\sum_{s \\in S}sim_{s,u} \\cdot (R_{s,p} - \\overline R_s)}{\\sum_{s \\in S}sim_{s,u}}$$\nFinally, after ranking $R_{u,p}$ for all possible items $p$, we can recommend the top k items to the current user.\nAnalysis  When the number of items is large, the overlap between the interaction histories of two users will be so small that it\u0026rsquo;s hard to calculate the similarity between them. UserCF requires maintaining a super large user similarity matrix to get the top N similar users fast which is extremely space consuming. UserCF is more suitable when the number of users is small and the number of items is large. As it is based on user similarity, it can help users to find their potential interests. Thus it may perform better when the interests of users change quickly, e.g. in the context of news recommendations.   ItemCF ItemCF recommends the top N similar items of the current item based on user historical behaviors, i.e. recommends the items that were usually bought by the user who also bought the current item.\nSteps 1. Calculate similarities between items. Similar to UserCF, we can use Cosine Similarity and Pearson Correlation Coefficient to calculate the similarity between two items.\nAlso, we can use a simpler equation $sim_{p,q} = \\frac{|N(p) \\cap N(q)|}{|N(p)|}$ to calculate the similarity, where $N(p)$ denotes the set of users who bought item $p$.\n2. Calculate ratings of related items and recommend the top K of them. The predicted rating of current item $p$ by user $u$ can be calculated by:\n$$R_{u,p} = \\overline R_p + \\frac{\\sum_{q\\in S}sim_{p,q}(R_{u,q} - \\overline R_{q})}{\\sum_{q\\in S}sim_{p,q}}$$\nwhere S denotes the set of top N similar items of item $p$.\nOptimize the similarity equation We\u0026rsquo;ll take the simple similarity equation $sim_{p,q} = \\frac{|N(p) \\cap N(q)|}{|N(p)|}$ as an example.\nAdd penalties for popular items $$sim_{p,q} = \\frac{|N(p) \\cap N(q)|}{\\sqrt{|N(p)|\\cdot |N(q)|}}$$\nThe more popluar an item is, the more penalty it will get.\nControl penalties for popular items $$sim_{p,q} = \\frac{|N(p) \\cap N(q)|}{|N(p)|^{1-\\alpha}\\cdot |N(q)|^\\alpha}$$\nWe can introduce a hyperparameter $\\alpha$ to control the penalties for popular items.\nAdd penalties for extremely active users $$sim_{p,q} = \\frac{1}{|N(p)|^{1-\\alpha}\\cdot |N(q)|^\\alpha}\\sum_{u \\in {N(p) \\cap N(q)}} \\frac{1}{\\log (1+|I(u)|)}$$\n$I(u)$ here denotes the set of items bought by user $u$. For an unusually active user, his/her contribution should be less than that of an inactive user when calculating the similarity between items.\nAnalysis  ItemCF is more suitable when the number of items is small and the number of users is large and users\u0026rsquo; interests are stable, e.g. movie and music recommendations.  Conclusion  One of the pros of Collaborative Filtering is that it only requires user-item interaction history as input. However, it\u0026rsquo;s also one of the cons because it cannot utilize other items and user features. Popular items are likely to be similar to a large number of items, while tail items are rarely recommended due to sparse interactive vectors.   References [1] Ruyi Luo et al. FunRec - UserCF\n[2] Ruyi Luo et al. FunRec - ItemCF\n[3] Ruyi Luo et al. FunRec - Swing\n","permalink":"https://wp-zhang.github.io/posts/collaborative-filtering/","summary":"UserCF UserCF uses ratings of the target item from top N similar users to predict the rating from the current user. There are mainly two steps:\nSteps 1. Calculate similarities between users. There are three common ways of calculating user similarities:\na. Jaccard Similarity: $$J_{u,v} = \\frac{|N(u) \\cap N(v)|}{|N(u) \\cup N(v)|}$$ $N(u)$ denotes the interacted item set of user $u$.\nb. Cosine Similarity: $$cos(u,v) = \\frac{u\\cdot v}{|u|\\cdot|v|}$$ $u$ and $v$ denote the rating vectors of two users respectively.","title":"Retrieval 01: Collaborative Filtering"},{"content":"Feature Engineering 1. Why do we need to apply normalization to numerical features? There are two common ways of normalization:\na. Min-Max Scaling\n$$X_{norm} = \\frac{X-X_{min}}{X_{max}-X_{min}}$$\nThis method can scale the data into a range of [0,1).\nb. Z-Score Normalization\n$$z = \\frac{x-\\mu}{\\rho}, \\quad \\rho=\\sqrt{\\sum\\frac{(x_i-\\mu)^2}{N}}$$\nThis method will scale the data and make the mean value and standard deviation of the new data become 0 and 1 respectively.\nWhen the scales of features are different, the gradients of weights of features can be very different, leading to a different \u0026rsquo;learning pace\u0026rsquo; of each weight, shown as a zig-zag on the gradient plot. Scaling features can make the \u0026rsquo;learning pace\u0026rsquo; of each weight become closer and let the model converge faster.\nScaling the features can be useful when the model uses gradient descent to update the parameters. Otherwise, it may not make a difference, e.g. when the model is a decision tree.\n2. How do we handle categorical features? We can use Ordinal Encoding, One-hot Encoding, and Binary Encoding based on the categorical feature itself. In deep learning, we can learn an embedding representation of a categorical feature, which can be seen as an advanced way of One-hot Encoding and Binary Encoding.\nWe can further encode categorical features with other features, e.g. statistical values of numerical features of each category, and target encoding.\n3. What is Feature Intersection? How do we intersect high-dimensional features? We can intersect different features and generate new features to improve the fitting ability of our model.\nFor high-dimensional features, we can first represent them in lower dimensions and then intersect them. A typical case is in the context of recommendation, we use matrix factorization to represent users and items in lower dimension vectors and then intersect the vectors to represent the relationship between users and items.\n4. How to efficiently find the features to be intersected?  We can intersect high-importance features based on the feature importance provided by models. We can generate intersected features with practical meanings, e.g. dividing price by area can get the price per sqft. We can train a decision tree and get inspiration from the structure of the trained tree. Every path from the root node to a leaf node can be seen as a way of intersecting features.  5. List some text representation models as well as their advantages and disadvantages. 6. How does Word2Vec work? What\u0026rsquo;s the difference between it and LDA? 7. What problem will insufficient data bring in the context of image classification? How can we solve it? Insufficient data may lead to model overfitting. To solve the problem, we can:\n Simplify the model, add regularization, and dropout. Data augmentation by image rotation, flipping, shifting, etc. Transfer learning. Fine-tune a pre-trained model trained on the small dataset.  Model Evaluation  The limitation of metrics.\n 1. What\u0026rsquo;s the limitation of accuracy? When the positive samples and negative samples are imbalanced, accuracy may not correctly reflect the performance of the model.\n2. How do we balance precision and recall rate? We can use the Precision-Recall curve, ROC or F1 score to evaluate the performance of a ranking/classification model.\n$$F1 = \\frac{2\\times precision \\times recall}{precision + recall}$$\n3. The RMSE of the model is high but 95% of samples in the test set are predicted with a small error, why is that? How can we solve it? RMSE will be heavily influenced by outliers in the dataset. We can remove these outliers if they are considered noise. Otherwise, we have to improve our model. We can also use more robust metrics like Mean Absolute Percent Error (MAPE): $\\sum_{i=1}^n |\\frac{y_i - \\hat y_i}{y_i} | \\times \\frac{100}{n}$.\n ROC curve.\n 4. What is the ROC curve? How do we draw it? The x-axis of a ROC curve denotes the False Positive Rate $FPR=\\frac{FP}{N}$ and the y-axis denotes the True Positive Rate $TPR = \\frac{TP}{P}$.\nA ROC curve can be drawn by keeping changing the threshold and getting the FPR-TPR pairs. Or we can first sort the samples by their predicted probabilities in descending order, then starting at (0,0) we iterate through all samples, moving upward if the sample is positive and moving right if it is negative.\n5. What\u0026rsquo;s the difference between the ROC curve and the P-R curve? The shape of the ROC curve remains almost the same when the distribution of positive and negative samples changes while the shape of the P-R curve varies in different datasets. Thus the ROC curve is more stable and can be applied in many different fields.\n","permalink":"https://wp-zhang.github.io/posts/cracking-the-machine-learning-interview/","summary":"Feature Engineering 1. Why do we need to apply normalization to numerical features? There are two common ways of normalization:\na. Min-Max Scaling\n$$X_{norm} = \\frac{X-X_{min}}{X_{max}-X_{min}}$$\nThis method can scale the data into a range of [0,1).\nb. Z-Score Normalization\n$$z = \\frac{x-\\mu}{\\rho}, \\quad \\rho=\\sqrt{\\sum\\frac{(x_i-\\mu)^2}{N}}$$\nThis method will scale the data and make the mean value and standard deviation of the new data become 0 and 1 respectively.\nWhen the scales of features are different, the gradients of weights of features can be very different, leading to a different \u0026rsquo;learning pace\u0026rsquo; of each weight, shown as a zig-zag on the gradient plot.","title":"Cracking the Machine Learning Interview"},{"content":"\nHi there~\nI\u0026rsquo;m Weipeng Zhang and you can call me Y.Paang (much easier to pronounce).\nI\u0026rsquo;m currently an MSDS student at Northeastern University, Boston, US. I\u0026rsquo;m a machine learning and data mining enthusiast and have work experience in data engineering.\nI love to spend my spare time participating in Kaggle competitions. I\u0026rsquo;m currently a Kaggle Competition Expert and have won 2 silver medals (one recently) so far.\n","permalink":"https://wp-zhang.github.io/about/","summary":"Hi there~\nI\u0026rsquo;m Weipeng Zhang and you can call me Y.Paang (much easier to pronounce).\nI\u0026rsquo;m currently an MSDS student at Northeastern University, Boston, US. I\u0026rsquo;m a machine learning and data mining enthusiast and have work experience in data engineering.\nI love to spend my spare time participating in Kaggle competitions. I\u0026rsquo;m currently a Kaggle Competition Expert and have won 2 silver medals (one recently) so far.","title":"About"}]